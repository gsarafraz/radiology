{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.Preprocessing import Preprocessing\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "data_path = '/Users/User/Desktop/radiology/data/'\n",
    "preprocessing = Preprocessing(data_folder = data_path)\n",
    "# data =  pd.read_csv(data_path + 'Radiology.csv')\n",
    "# data['cleaned_report'] = data['توضیحات پزشک'].apply(lambda x: preprocessing.cleaner(x))\n",
    "# data['cleaned_report'] = data['cleaned_report'].apply(lambda x: preprocessing.complete_normalization(x))\n",
    "# data['tokenized_report'] = data['cleaned_report'].apply(lambda x: preprocessing.sentence_tokenizer(x))\n",
    "# data.to_excel(data_path + 'cleaned_tokenized_report.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = list(pd.read_csv(data_path + 'diseases_third_version.csv')['Term'])\n",
    "base_types_diseases = list(pd.read_csv(data_path + 'diseases_third_version.csv')['baseType'])\n",
    "body_parts = list(pd.read_csv(data_path + 'body_parts_second_version.csv')['Term'])\n",
    "diseases = list([preprocessing.complete_normalization(x) for x in diseases])[1:]\n",
    "base_types_diseases = list([preprocessing.complete_normalization(x) for x in base_types_diseases])[1:]\n",
    "body_parts = list(set([preprocessing.complete_normalization(x) for x in body_parts]))[1:]\n",
    "\n",
    "diseases_df = pd.DataFrame({'term':diseases, 'baseType': base_types_diseases})\n",
    "body_parts_df = pd.DataFrame({'term':body_parts})\n",
    "\n",
    "diseases_df_tmp = pd.DataFrame()\n",
    "\n",
    "for index, row in diseases_df.iterrows():\n",
    "    term = row['term']\n",
    "    baseType = row['baseType']\n",
    "    words = term.split()\n",
    "    for word in words:\n",
    "        diseases_df_tmp2 = {'term': [term.replace(word, word + 'های'), term.replace(word, word + 'ی'), term.replace(word, word + 'ها')], 'baseType': [baseType for i in range(3)]}\n",
    "        diseases_df_tmp2 = pd.DataFrame(data=diseases_df_tmp2)\n",
    "        diseases_df_tmp = pd.concat([diseases_df_tmp, diseases_df_tmp2], axis=0)\n",
    "    \n",
    "diseases_df = pd.concat([diseases_df_tmp, diseases_df], axis=0)\n",
    "\n",
    "def count_subwords(word):\n",
    "    return len(word.split())\n",
    "\n",
    "diseases_df['num_subwords'] = diseases_df['term'].apply(lambda x: count_subwords(x))\n",
    "body_parts_df['num_subwords'] = body_parts_df['term'].apply(lambda x: count_subwords(x))\n",
    "\n",
    "max_subwords = max(body_parts_df['num_subwords'].max(), diseases_df['num_subwords'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "neg_verbs = ['نشد', 'نمیشود','نمی‌شود','نمی شود', 'ندارند','نبود','نگردد','نباشد', 'نگردید', 'نمیتواند', 'نمی‌تواند','ندارد','نمی تواند','نیست','نمی']\n",
    "\n",
    "def find_polarity(sent):\n",
    "    \n",
    "    for verb in neg_verbs:\n",
    "        if(verb in sent):\n",
    "            return 'منفی'\n",
    "    return 'مثبت'\n",
    "    \n",
    "    \n",
    "def find_shared_body_parts_and_diseases(sent):\n",
    "    \n",
    "    shared_body_parts = []\n",
    "    shared_disease = []\n",
    "    shared_disease_bt = []\n",
    "    count_subwords = max_subwords\n",
    "    \n",
    "    while(count_subwords > 0):\n",
    "        \n",
    "        n_grams = ngrams(sequence = sent.split(), n = count_subwords)\n",
    "        grams_list = []\n",
    "        for grams in n_grams:\n",
    "            grams_list.append(' '.join(grams))\n",
    "            \n",
    "        grams_list = list(set(grams_list))\n",
    "            \n",
    "        shared_bodyparts_tmp  = list(body_parts_df[(body_parts_df['num_subwords']  == count_subwords) & (body_parts_df['term'].isin(grams_list))]['term'])\n",
    "        shared_diseases_tmp_bt = list(diseases_df[(diseases_df['num_subwords'] == count_subwords) & (diseases_df['term'].isin(grams_list))]['baseType'])\n",
    "        shared_diseases_tmp = list(diseases_df[(diseases_df['num_subwords'] == count_subwords) & (diseases_df['term'].isin(grams_list))]['term'])\n",
    "        \n",
    "        for word in shared_bodyparts_tmp:\n",
    "            sent = sent.replace(word, '')\n",
    "            \n",
    "        for word in shared_diseases_tmp:\n",
    "            sent = sent.replace(word, '')\n",
    "            \n",
    "        shared_body_parts += shared_bodyparts_tmp\n",
    "        shared_disease += shared_diseases_tmp\n",
    "        shared_disease_bt += shared_diseases_tmp_bt\n",
    "        count_subwords -= 1\n",
    "    \n",
    "    return shared_body_parts, shared_disease, shared_disease_bt\n",
    "\n",
    "def find_prev_tag(sentenceslist, sentindx):\n",
    "    \n",
    "    tag = None\n",
    "    sentenceslist = sentenceslist[:sentindx]\n",
    "    for i in range(1,sentindx+1):\n",
    "        sent = sentenceslist[-1*i]\n",
    "        if('tag' in sent):\n",
    "            intersect = list(set(sent.split()).intersection(body_parts))\n",
    "            if(len(intersect) > 0):\n",
    "                tag = intersect[0]\n",
    "            break\n",
    "    return tag\n",
    "    \n",
    "    \n",
    "def bodypart_disease_alignment(sentenceslist):\n",
    "    all_results = []\n",
    "    \n",
    "    for sentindx, sent in enumerate(sentenceslist):\n",
    "        result = []\n",
    "        print(\"****************************\")\n",
    "        print(sent)\n",
    "        shared_bodyparts, shared_diseases, shared_diseases_bt = find_shared_body_parts_and_diseases(sent)\n",
    "        polarity = find_polarity(sent)\n",
    "        \n",
    "        if(len(shared_diseases) != 0):\n",
    "            \n",
    "            if(len(shared_bodyparts) == 0):\n",
    "                tag = find_prev_tag(sentenceslist, sentindx)\n",
    "                print(tag)\n",
    "                print(shared_diseases_bt[index])\n",
    "                for index, disease in enumerate(shared_diseases):\n",
    "                        result.append({'عارضه':shared_diseases_bt[index],'عضو':tag, 'رخداد یا عدم رخداد': polarity})\n",
    "                \n",
    "            else:\n",
    "                index_bps = [sent.index(x) for x in shared_bodyparts]\n",
    "                index_diseases = [sent.index(x) for x in shared_diseases]\n",
    "        \n",
    "                for index, d_index in enumerate(index_diseases):\n",
    "                    \n",
    "                    tmp_arr = np.abs(np.array(index_bps) - d_index)\n",
    "                    bp_min_index = np.argmin(tmp_arr)\n",
    "                    print(bp_min_index)\n",
    "                    print(shared_bodyparts[bp_min_index])\n",
    "                    print(shared_diseases_bt[index])\n",
    "                    result.append({'عارضه':shared_diseases_bt[index],'عضو':shared_bodyparts[bp_min_index], 'رخداد یا عدم رخداد': polarity})\n",
    "        \n",
    "        if(len(result) > 0):\n",
    "            all_results.append(result)\n",
    "    return all_results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.Preprocessing import Preprocessing\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "data_path = '/Users/User/Desktop/radiology/data/'\n",
    "preprocessing = Preprocessing(data_folder = data_path)\n",
    "\n",
    "def summarize_radiology_report():\n",
    "    report = inputtxt.get(1.0, \"end-1c\")\n",
    "    report = preprocessing.cleaner(report)\n",
    "    \n",
    "    print(report)\n",
    "    print(\"###\")\n",
    "    report = preprocessing.complete_normalization(report)\n",
    "    print(report)\n",
    "    print(\"####\")\n",
    "    report = preprocessing.sentence_tokenizer(report)\n",
    "    \n",
    "    lbl.config(text = str(bodypart_disease_alignment(report)).replace('],', '] \\n'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:  قفسه صدری دیگر 1 فیلم رادیوگرافی ساده شکم خوابیده ایستاده 2 فیلم رادیوگرافی ریه: کاردیومگالی دارد. برجستگی کلسیفیکیشن قوس ایورت مشهود است. djd توراسیک تصویر کدورت های رتیکولر ریتین بویژه زون های 2 رویت میشود. تشخیص های افتراقی aspiration pneumonia atypic pneumonia درجه بیمار مطرح میکند. رادیوگرافی شکم: تصویر سنگ مسیر سیستم ادراری مشاهده نشد. کلسیفیکیشن شکم وجود ندارد. گاز روده طبیعی است. djd لومبار مشهود است. لذا تطابق یافته.های بالینی ازمایشگاهی نظر تایید تشخیص اقدامات مقتضی توصیه میگردد.\n",
      "****************************\n",
      "قفسه صدری دیگر 1 فیلم رادیوگرافی ساده شکم خوابیده ایستاده 2 فیلم رادیوگرافی ریه tag  .\n",
      "****************************\n",
      "کاردیومگالی دارد .\n",
      "****************************\n",
      " برجستگی کلسیفیکیشن قوس ایورت مشهود است .\n",
      "0\n",
      "قوس ایورت\n",
      "کلسیفیکیشن\n",
      "****************************\n",
      " djd توراسیک تصویر کدورت های رتیکولر ریتین بویژه زون های 2 رویت میشود .\n",
      "شکم\n",
      "ارتروز\n",
      "****************************\n",
      " تشخیص های افتراقی aspiration pneumonia atypic pneumonia درجه بیمار مطرح میکند .\n",
      "****************************\n",
      " رادیوگرافی شکم tag  .\n",
      "****************************\n",
      "تصویر سنگ مسیر سیستم ادراری مشاهده نشد .\n",
      "0\n",
      "سیستم ادراری\n",
      "سنگ\n",
      "****************************\n",
      " کلسیفیکیشن شکم وجود ندارد .\n",
      "0\n",
      "شکم\n",
      "کلسیفیکیشن\n",
      "****************************\n",
      " گاز روده طبیعی است .\n",
      "****************************\n",
      " djd لومبار مشهود است .\n",
      "شکم\n",
      "ارتروز\n",
      "****************************\n",
      " لذا تطابق یافته .\n",
      "****************************\n",
      "های بالینی ازمایشگاهی نظر تایید تشخیص اقدامات مقتضی توصیه میگردد .\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "\n",
    "# Top level window\n",
    "frame = tk.Tk()\n",
    "frame.title(\"TextBox Input\")\n",
    "frame.geometry('1000x500')\n",
    "\n",
    "# TextBox Creation\n",
    "inputtxt = tk.Text(frame,\n",
    "                   height = 20,\n",
    "                   width = 800)\n",
    "  \n",
    "inputtxt.pack()\n",
    "  \n",
    "# Button Creation\n",
    "printButton = tk.Button(frame,\n",
    "                        text = \"Summarize The Report\", \n",
    "                        bg='blue',\n",
    "                        fg='red',\n",
    "                        command = summarize_radiology_report)\n",
    "\n",
    "printButton.pack()\n",
    "  \n",
    "# Label Creation\n",
    "lbl = tk.Label(frame, text = \"\")\n",
    "lbl.pack()\n",
    "frame.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_excel(data_path + 'cleaned_tokenized_report.xlsx')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.iloc[0]['tokenized_report'])\n",
    "# bodypart_disease_alignment(data.iloc[0]['tokenized_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.iloc[20]['توضیحات پزشک'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "# import numpy as np\n",
    "\n",
    "# model_name_or_path = \"persiannlp/mt5-large-parsinlu-sentiment-analysis\"\n",
    "# tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "# model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# def model_predict(text_a, text_b):\n",
    "#     features = tokenizer( [(text_a, text_b)], padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "#     output = model(**features)\n",
    "#     logits = output[0]\n",
    "#     probs = torch.nn.functional.softmax(logits, dim=1).tolist()\n",
    "#     idx = np.argmax(np.array(probs))\n",
    "#     print(labels[idx], probs)\n",
    "\n",
    "\n",
    "# def run_model(context, query, **generator_args):\n",
    "#     input_ids = tokenizer.encode(context + \"<sep>\" + query, return_tensors=\"pt\")\n",
    "#     res = model.generate(input_ids, **generator_args)\n",
    "#     output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "#     print(output)\n",
    "#     return output\n",
    "\n",
    "\n",
    "# run_model(\n",
    "#     \"یک فیلم ضعیف بی محتوا بدون فیلمنامه . شوخی های سخیف .\",\n",
    "#     \"نظر شما در مورد داستان، فیلمنامه، دیالوگ ها و موضوع فیلم  لونه زنبور چیست؟\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import parsivar\n",
    "\n",
    "# data_path = '/Users/User/Desktop/radiology/data/'\n",
    "# sentences = pd.read_excel(data_path + 'unique_sentences.xlsx')\n",
    "\n",
    "# all_verbs = []\n",
    "\n",
    "\n",
    "# for index, item in sentences.iterrows():\n",
    "    \n",
    "#     sample = item['sentence']\n",
    "#     my_tokenizer = parsivar.Tokenizer()\n",
    "#     my_tagger = parsivar.POSTagger(tagging_model=\"wapiti\")  \n",
    "#     tags = my_tagger.parse(my_tokenizer.tokenize_words(sample.translate(str.maketrans('', '', string.punctuation))))\n",
    "#     my_chunker = parsivar.FindChunks()\n",
    "#     chunks = my_chunker.chunk_sentence(tags)\n",
    "#     all_chunks = my_chunker.convert_nestedtree2rawstring(chunks)\n",
    "#     all_chunks = all_chunks.split(']')\n",
    "#     func = lambda x: x.replace('VP','').replace(']','').replace('[','').strip() if ('VP' in x) else None\n",
    "#     verbs = np.array([func(tag) for tag in all_chunks])\n",
    "#     verbs = set(verbs[verbs != None])\n",
    "#     all_verbs += list(verbs)\n",
    "        \n",
    "# all_verbs = list(set(all_verbs))\n",
    "# all_verbs\n",
    "# # def find_verbs(sent):\n",
    "# #     sent = set(sent.split())\n",
    "# #     return len(sent.intersection(set(body_parts)))\n",
    "\n",
    "# # def find_adject(sent):\n",
    "# #     sent = set(sent.split())\n",
    "# #     return sent.intersection(set(body_parts))\n",
    "\n",
    "# # sentences['num_body_parts'] = sentences['sentence'].apply(lambda x:find_body_parts_len(x))\n",
    "# # sentences['body_parts'] = sentences['sentence'].apply(lambda x:find_body_parts(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/Users/User/Desktop/radiology/data/'\n",
    "# sentences = pd.read_excel(data_path + 'unique_sentences.xlsx')\n",
    "# all_adj = []\n",
    "# all_verbs = []\n",
    "\n",
    "\n",
    "# for index, item in sentences.iterrows():\n",
    "    \n",
    "#     sample = item['sentence']\n",
    "#     my_tokenizer = parsivar.Tokenizer()\n",
    "#     my_tagger = parsivar.POSTagger(tagging_model=\"wapiti\")  \n",
    "#     tags = my_tagger.parse(my_tokenizer.tokenize_words(sample.translate(str.maketrans('', '', string.punctuation))))\n",
    "#     func = lambda x: x[0] if ('V' in x[1]) else None\n",
    "#     adjs = np.array([func(tag) for tag in tags])\n",
    "#     all_adj += list(adjs)\n",
    "    \n",
    "        \n",
    "# all_adj = list(set(all_adj))\n",
    "# print(len(all_adj))\n",
    "# all_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
